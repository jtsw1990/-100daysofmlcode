{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:00:50.663845Z",
     "start_time": "2018-11-11T13:00:50.022305Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:00:50.688181Z",
     "start_time": "2018-11-11T13:00:50.679988Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:00:50.720697Z",
     "start_time": "2018-11-11T13:00:50.715836Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:00:50.873520Z",
     "start_time": "2018-11-11T13:00:50.869045Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.concat([pd.DataFrame(boston['data']), pd.DataFrame(boston['target'])], axis = 1)) \n",
    "df.columns = list(boston['feature_names']) + ['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:00:51.052927Z",
     "start_time": "2018-11-11T13:00:51.046751Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_nn = df[['LSTAT', 'target']]\n",
    "x = np.array(df_nn['LSTAT']).reshape(506, 1)\n",
    "y = np.array(df_nn['target']).reshape(506, 1)\n",
    "test = np.concatenate([x, y], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "FIND OUT ABOUT NAME SPACES FOR FUNCTIONS IN CLASSES FOR ACTIVATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualizing Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:01:07.204334Z",
     "start_time": "2018-11-11T13:00:51.566574Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s 480us/step - loss: 3.6807 - acc: 0.5951\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.9300 - acc: 0.6003\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.7462 - acc: 0.6380\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.7104 - acc: 0.6549\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.6811 - acc: 0.6784\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6505 - acc: 0.6810\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.6497 - acc: 0.6706\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.6360 - acc: 0.6875\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.6240 - acc: 0.6914\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6287 - acc: 0.6771\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.6472 - acc: 0.6732\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.6390 - acc: 0.6758\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6249 - acc: 0.6745\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.6179 - acc: 0.6992\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.6019 - acc: 0.6979\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5883 - acc: 0.7018\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5848 - acc: 0.6953\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 118us/step - loss: 0.6000 - acc: 0.6888\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5800 - acc: 0.7083\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5799 - acc: 0.7214\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5684 - acc: 0.7174\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5819 - acc: 0.6966\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5742 - acc: 0.7135\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.5676 - acc: 0.7318\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.5577 - acc: 0.7357\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5708 - acc: 0.7031\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5558 - acc: 0.7201\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5551 - acc: 0.7305\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5736 - acc: 0.7148\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 118us/step - loss: 0.5614 - acc: 0.7214\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5692 - acc: 0.7174\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5653 - acc: 0.7109\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5532 - acc: 0.7214\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5511 - acc: 0.7279\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5542 - acc: 0.7188\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5622 - acc: 0.7070\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5358 - acc: 0.7344\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5454 - acc: 0.7148\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5494 - acc: 0.7214\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5488 - acc: 0.7161\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 118us/step - loss: 0.5448 - acc: 0.7279\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5396 - acc: 0.7396\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5344 - acc: 0.7422\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5356 - acc: 0.7409\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5324 - acc: 0.7565\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5295 - acc: 0.7513\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5348 - acc: 0.7396\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5344 - acc: 0.7396\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5349 - acc: 0.7474\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 118us/step - loss: 0.5274 - acc: 0.7409\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5281 - acc: 0.7500\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.5299 - acc: 0.7435\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5396 - acc: 0.7435\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5392 - acc: 0.7266\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5232 - acc: 0.7513\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5286 - acc: 0.7448\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5337 - acc: 0.7383\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5232 - acc: 0.7539\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5137 - acc: 0.7643\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5358 - acc: 0.7461\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5254 - acc: 0.7409\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5170 - acc: 0.7565\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5415 - acc: 0.7383\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.5327 - acc: 0.7422\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5192 - acc: 0.7552\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5063 - acc: 0.7552\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5165 - acc: 0.7383\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5119 - acc: 0.7526\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5113 - acc: 0.7487\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5350 - acc: 0.7188\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.5179 - acc: 0.7474\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5148 - acc: 0.7578\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5151 - acc: 0.7539\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5095 - acc: 0.7669\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5089 - acc: 0.7617\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.5109 - acc: 0.7617\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.5142 - acc: 0.7630\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 149us/step - loss: 0.5105 - acc: 0.7474\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.5115 - acc: 0.7409\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5050 - acc: 0.7695\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5022 - acc: 0.7695\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5001 - acc: 0.7578\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 125us/step - loss: 0.4983 - acc: 0.7617\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.4948 - acc: 0.7591\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 138us/step - loss: 0.5035 - acc: 0.7565\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.5065 - acc: 0.7552\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 152us/step - loss: 0.4968 - acc: 0.7604\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.5000 - acc: 0.7643\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5025 - acc: 0.7773\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5098 - acc: 0.7539\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4967 - acc: 0.7604\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5099 - acc: 0.7500\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.4988 - acc: 0.7630\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.4959 - acc: 0.7695\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5016 - acc: 0.7526\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.4883 - acc: 0.7695\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.4966 - acc: 0.7773\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.4883 - acc: 0.7643\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.4885 - acc: 0.7682\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.4843 - acc: 0.7826\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.4883 - acc: 0.7734\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.4969 - acc: 0.7643\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.4967 - acc: 0.7591\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.4902 - acc: 0.7878\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5289 - acc: 0.7487\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.4901 - acc: 0.7760\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4885 - acc: 0.7747\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5018 - acc: 0.7630\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4863 - acc: 0.7604\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4855 - acc: 0.7721\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4828 - acc: 0.7826\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4879 - acc: 0.7773\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.4972 - acc: 0.7526\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.4882 - acc: 0.7656\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.4907 - acc: 0.7708\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.4895 - acc: 0.7695\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.4889 - acc: 0.7630\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.4851 - acc: 0.7839\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.4825 - acc: 0.7643\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.4935 - acc: 0.7786\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4902 - acc: 0.7773\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.4812 - acc: 0.7812\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4840 - acc: 0.7656\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4823 - acc: 0.7799\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.4832 - acc: 0.7839\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.4778 - acc: 0.7695\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.4853 - acc: 0.7695\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4694 - acc: 0.7773\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4779 - acc: 0.7773\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.4687 - acc: 0.7891\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4809 - acc: 0.7617\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.4790 - acc: 0.7812\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4812 - acc: 0.7721\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.4833 - acc: 0.7734\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.4720 - acc: 0.7734\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4702 - acc: 0.7799\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4663 - acc: 0.7852\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.4772 - acc: 0.7904\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.4653 - acc: 0.7760\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4777 - acc: 0.7813\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.4690 - acc: 0.7865\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4799 - acc: 0.7682\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.4733 - acc: 0.7826\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4720 - acc: 0.7865\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 118us/step - loss: 0.4864 - acc: 0.7617\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4917 - acc: 0.7695\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.4804 - acc: 0.7799\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.4674 - acc: 0.7773\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.4721 - acc: 0.7643\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.4769 - acc: 0.7799\n",
      "768/768 [==============================] - 0s 60us/step\n",
      "\n",
      "acc: 77.73%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "numpy.random.seed(7)\n",
    "\n",
    "dataset = numpy.loadtxt(\"exampleDatasets/pima-indians-diabetes.csv\", delimiter = \",\")\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 8, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=150, batch_size=10)\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:01:07.361972Z",
     "start_time": "2018-11-11T13:01:07.358447Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "#ann_viz(model, view = True, title=\"My first neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NeuralNetwork Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:12:22.794143Z",
     "start_time": "2018-11-11T13:12:22.783906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, architecture):\n",
    "        '''Lets take a list as an input to specify the architecture of our NN\n",
    "        For example, [2, 3, 1] will be a network with 2 inputs, 1 hidden layer of 3 neurons and\n",
    "        a single output'''\n",
    "        \n",
    "        self.structure = architecture\n",
    "        self.num_layers = len(self.structure)\n",
    "        self.bias = [np.random.rand(layer, 1)for layer in self.structure[1:]]\n",
    "        self.weights = [np.random.rand(dimensions[0], dimensions[1]) for dimensions in zip(self.structure[1:], self.structure[:-1])]\n",
    "        \n",
    "    def activationFunction(self, x, derivative = False):\n",
    "        if derivative:\n",
    "            return 1\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def feedForward(self, inputs):\n",
    "        '''Feed inputs all into the output neurons but saves all the layers for backpropagation'''\n",
    "        \n",
    "        layer = 0\n",
    "        layer_outputs = []\n",
    "        while layer < self.num_layers - 1:\n",
    "            if layer == 0:\n",
    "                prev_layer = inputs\n",
    "                next_layer = self.activationFunction(np.dot(inputs, self.weights[layer].T).T + self.bias[layer], \n",
    "                                                        derivative = False).T\n",
    "                layer_outputs.append(next_layer)\n",
    "                layer += 1\n",
    "            else:\n",
    "                prev_layer = next_layer\n",
    "                next_layer = self.activationFunction(np.dot(prev_layer, self.weights[layer].T).T + self.bias[layer], \n",
    "                                                        derivative = False).T\n",
    "                layer_outputs.append(next_layer)\n",
    "                layer += 1\n",
    "        return next_layer, layer_outputs\n",
    "        \n",
    "    def calculate_cost(self, response, outputs, derivative = False):\n",
    "        '''This assumes a quadratic cost function at the moment'''\n",
    "        if derivative:\n",
    "            return (outputs - response)\n",
    "        else:\n",
    "            return np.sum(0.5 * (response - outputs) ** 2) * (1 / len(response))\n",
    "        \n",
    "    \n",
    "    def backPropagate(self):\n",
    "        '''Break function up into 3 parts\n",
    "        Back propagate\n",
    "        update weights'''\n",
    "        \n",
    "        learning_rate = 0.01\n",
    "        delta_k = activationFunction(outputs, derivative = True) * d_errors # First calculate delta_k in the output node\n",
    "                                                                            # This should be shaped same as errors, so (nx1)\n",
    "        for layer in reversed(range(net.num_layers)): # net.num_layer = 3, reverse range of that = 2, 1, 0\n",
    "            if layer == (net.num_layers - 1):\n",
    "                d_weights = np.dot(delta_k.T, prev_layers[layer - 1])\n",
    "                d_bias = np.dot(delta_k.T, np.ones(506).reshape(506, 1))\n",
    "                net.weights[layer - 1] -= learning_rate * d_weights\n",
    "                net.bias[layer - 1] -= learning_rate * d_bias\n",
    "\n",
    "            else:\n",
    "                # Add a loop for rest of layers for k in reversed(range(net.num_layer - 1)):\n",
    "                # Works the first time because it breaks on the input layer\n",
    "                delta_j = activationFunction(prev_layers[1], derivative = True) * np.dot(delta_k, net.weights[1])\n",
    "                d_weights = np.dot(delta_j.T, prev_layers[np.max(0, 0)])\n",
    "                d_bias = np.dot(delta_j.T, np.ones(506).reshape(506, 1))\n",
    "                net.weights[np.max(0, 0)] -= (d_weights * learning_rate)\n",
    "                net.bias[np.max(0, 0)] -= (learning_rate * d_bias)\n",
    "\n",
    "        \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:01:07.703087Z",
     "start_time": "2018-11-11T13:01:07.699657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork([2, 3, 1])\n",
    "outputs = net.feedForward(test)[0]\n",
    "prev_layers = net.feedForward(test)[1]\n",
    "prev_layers.insert(0, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:01:07.903449Z",
     "start_time": "2018-11-11T13:01:07.900438Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "errors = net.calculate_cost(y, outputs)\n",
    "d_errors = net.calculate_cost(y, outputs, derivative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:01:08.104332Z",
     "start_time": "2018-11-11T13:01:08.102017Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def activationFunction(x, derivative = False):\n",
    "     if derivative:\n",
    "        return 1\n",
    "     else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:11:21.795156Z",
     "start_time": "2018-11-11T13:11:21.790362Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def backPropagation():\n",
    "    learning_rate = 0.01\n",
    "    delta_k = activationFunction(outputs, derivative = True) * d_errors # First calculate delta_k in the output node\n",
    "                                                                        # This should be shaped same as errors, so (nx1)\n",
    "    for layer in reversed(range(net.num_layers)): # net.num_layer = 3, reverse range of that = 2, 1, 0\n",
    "        if layer == (net.num_layers - 1):\n",
    "            d_weights = np.dot(delta_k.T, prev_layers[layer - 1])\n",
    "            d_bias = np.dot(delta_k.T, np.ones(506).reshape(506, 1))\n",
    "            net.weights[layer - 1] -= learning_rate * d_weights\n",
    "            net.bias[layer - 1] -= learning_rate * d_bias\n",
    "            \n",
    "        else:\n",
    "            # Add a loop for rest of layers for k in reversed(range(net.num_layer - 1)):\n",
    "            # Works the first time because it breaks on the input layer\n",
    "            delta_j = activationFunction(prev_layers[1], derivative = True) * np.dot(delta_k, net.weights[1])\n",
    "            d_weights = np.dot(delta_j.T, prev_layers[np.max(0, 0)])\n",
    "            d_bias = np.dot(delta_j.T, np.ones(506).reshape(506, 1))\n",
    "            net.weights[np.max(0, 0)] -= (d_weights * learning_rate)\n",
    "            net.bias[np.max(0, 0)] -= (learning_rate * d_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T13:11:22.306912Z",
     "start_time": "2018-11-11T13:11:22.303279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "backPropagation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
