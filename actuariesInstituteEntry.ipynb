{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet, we will explore something that does not really fall into the realm of traditional actuarial problems, but still extremely useful nonetheless. This article will go through the process of text classification. <br><br> The dataset used can be found at: <br>\n",
    "https://www.kaggle.com/sebastienverpile/consumercomplaintsdata <br>\n",
    "https://catalog.data.gov/dataset/consumer-complaint-database <br>\n",
    "and consists of finance related complaints that a company has recieved from its customers. <br><br> This problem will be split into 2 parts; transforming text data into usable inputs, and performing classification with those inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages that we will be using are listed below. <br>\n",
    "Pandas and Numpy for general data manipulation <br>\n",
    "Matplotlib and Seaborn for general data visualisation <br>\n",
    "Sci-kit learn packages for both feature extraction and classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.487Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 - Importing and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are trying to predict the category of products based on the narrative of the complaints recieved, we can ignore the rest of the columns for the purpose of this exercise and only look at the single feature and target columns. We will also get rid of null entries as they will not be of any use to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.491Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Consumer_Complaints.csv')\n",
    "df = df.loc[(df['Consumer complaint narrative'].notnull()), ['Consumer complaint narrative', 'Product']] \\\n",
    "       .reset_index() \\\n",
    "       .drop('index', axis = 1)\n",
    "df = df[(np.logical_not(df.Product.str.contains(','))) & (df.Product != 'Credit card or prepaid card')]\n",
    "df.columns = ['description', 'target']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will assign each target variable an integer value instead of using the string, which allows our models to be able to read our responses. We can do this using a variety of methods, but here we will be using sklearn's LabelEncoder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.493Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(df.target)\n",
    "df = df.assign(encoded_response = lambda x: encoder.transform(x.target))\n",
    "df[['target', 'encoded_response']].drop_duplicates() \\\n",
    "                                  .set_index('target') \\\n",
    "                                  .sort_values('encoded_response', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above output that our resulting dataset contains 14 unique categories which we will try to classify complaints into by training our model to \"understand\" the narrative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split our dataset for training and validation purposes. Regardless of technique used, train test splits are always considered to be good practice and is extremely helpful to prevent over-fitting of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.496Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, indices_train, indices_test = train_test_split(df.description, \n",
    "                                                                                 df.encoded_response,\n",
    "                                                                                 df.index,\n",
    "                                                                                 test_size = 0.2, \n",
    "                                                                                 random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Text to features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are many examples of machine learning models that make use of a variety of inputs such as; images ect. we must understand that ultimately, whatever input chosen must be converted into numerical features in order to be \"understood\" by the model. That being said, images are made up of pixels, which can generally be represented by a integer triplet (x, y, z) indicating the intensity of RGB respectively. <br> In this section, we will find a meaningful way to represent our text data as numerical features. This is also known as feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of transforming a document full of text into numerical features is using the BOW (Bag Of Words) model. Put simply, all it does is assign each word (or token) an ID number, with its respective count. For example, if we have a document string <br>\n",
    "\"This is a cat. That is a dog.\"\n",
    "<br>\n",
    "The BOW representation would simply be {\"This\": 1, \"is\": 2, \"a\": 2, \"cat\" : 1, \"That\" : 1, \"dog\" : 1}\n",
    "<br>\n",
    "From the example above we can see that the BOW model just represents a document as its individual component words with its respective count attached to it. Notice that the document shown above is clearly about a cat and a dog. However, our BOW model shows that the most frequent words present in the document is \"is\" and \"a\". This is where the importance of data preprocessing is more obvious compared to other machine learning techniques. These common words are also known as \"stop words\" and are usually taken out of the BOW model to prevent overpowering the words that have actual importance. There are many different little tricks and techniques for choosing the most suitable bag of words to represent your documents and most can be implemented simply through a line (or two) of code using [regular expressions](https://docs.python.org/2/library/re.html).<br>\n",
    "How does this link to having a numerical matrix as an input? Well, imagine that we have a thousand text documents. We would choose (or use all of) the set of unique words present in that 1000 documents and give word a unique ID number, and make the our columns for the input matrix. For example, if we had chosen 15,000 unique tokens from the 1000 documents as our \"dictionary\", our input matrix would then have 15,000 columns! Each individual document will then represent a row in the (now extremely wide) matrix filled mostly with zeros, except for the columns corresponding to words found in that particular document, which will contain the count of that word. This technique is also commonly known as count vectorizing. <br>\n",
    "In the follow example however, we will be using a more robust model compared to count vectorizing called (TF-IDF) Term Frequency - Inverse Document Frequency, and it is defined as:\n",
    "\n",
    "$w_{i,j} = tf_{i,j} * log(\\dfrac{N}{df_i})$ \n",
    "<br><br>\n",
    "$w_{i,j}$ = Weight for word(i) in document(j) <br>\n",
    "$tf_{i,j}$ = Count of word(i) in document(j) <br>\n",
    "$N$ = Total number of documents <br>\n",
    "$df_i$ = How many documents word(i) appears in<br><br>\n",
    "\n",
    "We can see that, the first term of the TF-IDF model is just the count of the word in the document as before. The magic happens in the second term where the model imposes an additional condition for a particular word to be deemed \"important\". Just as an example, if the word \"bank\" appears in every single document, it wouldn't be of much use in differentiating the documents, and the second term of the TF-IDF model expresses this by reducing the whole weight down to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of packages that help to automate the vectorizing process but we have chosen to use the Sci-kit learn API due to its easy of usage and interpretability. Here, we instantiate a TFIDF model, with a few note-worthy details: <br>\n",
    "1) Sublinear_tf uses a logarithmic form of frequency as 20 occurrences of a word probably does not imply 20 times of importance <br>\n",
    "2) The model will ignore words that appear in less than 5 documents, as well as more than 70% of the total documents <br>\n",
    "3) Normalization is set to l2 so that all vectors are scaled to have a magnitude of 1 (The equivalent of standardizing your features) <br>\n",
    "4) There is a layer of preprocessing to remove numerical characters and symbols within the documents through the use of a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.499Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf = True, \n",
    "                                   stop_words = 'english',\n",
    "                                   min_df = 5,\n",
    "                                   max_df = 0.7,\n",
    "                                   norm = 'l2',\n",
    "                                   token_pattern = r'\\b[^_\\d\\W]+\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the model to fit our original text data, and convert all that sweet text information into something numerical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.501Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train.values)\n",
    "tfidf_test = tfidf_vectorizer.transform(x_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.503Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our resulting dictionary consists of 19,344 different unqiue words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.505Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_train.A, \n",
    "                        columns = tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.506Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the 19,344 words chosen to be in our \"dictionary\" become the columns of our input matrix and as you can probably guess, since we have so many columns and each row only consists of words within 1 document, the resulting matrix will be extremely sparse! (Consisting mostly of zeros) <br>\n",
    "Now that we have extracted some numerical features from our dataset, it is time to use them to train a classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have chosen to use a basic [logistic regression model](http://www.appstate.edu/~whiteheadjc/service/logit/intro.htm) to classify the documents due to tractability and convention. However, more complex models such as neural networks, decision trees, naive bayesian classifiers or other relevant models can also be used to do the following classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we instantiate a logistic regression classifier from the sklearn package and proceed to fit our vectorized matrix and our encoded training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.509Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.510Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = lr_classifier.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the model to predict the categories on the validation set, we can see that we attained some pretty decent predictive power with an overall accuracy of 85.2%. We can drill down into this result by evaluating the [confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.512Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.513Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "sns.heatmap(cm, \n",
    "            annot = True, \n",
    "            fmt = \".0f\", \n",
    "            linewidths = 0.5, \n",
    "            square = True, \n",
    "            cmap = 'Blues_r', \n",
    "            xticklabels = encoder.classes_,\n",
    "            yticklabels = encoder.classes_)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the obvious results we can observe from the matrix is that \"Other financial service\", \"Vehicle loan or lease\" and \"Virtual currency\" categories had no correct predictions. We must note however, that the number of datapoints we had for each of those categories were insignificant compared to the others and the model probably treated those instances as errors instead of a distinct category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bias however, can be corrected by techniques such as [over and undersampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis). Categories with more data points generally had a better score, and errors observed in classification seem to be pretty reasonable due to some overlap between the categories. (For example, 320 descriptions that were supposed to be \"Checking and savings account\" related were wrongly classified as \"Bank account and service\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.515Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def report2dict(cr):\n",
    "    '''Function purely for formatting purposes, can be ignored'''\n",
    "    \n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    measures = tmp[0]\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "            \n",
    "    return D_class_data\n",
    "\n",
    "\n",
    "results = report2dict(metrics.classification_report(y_test, pred, target_names = encoder.classes_))\n",
    "pd.DataFrame(results).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some examples where 'Checking and savings account' narratives were wrongly classified as 'Bank account and service'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.518Z"
    }
   },
   "outputs": [],
   "source": [
    "def observe_errors(actual_response, wrongly_predicted_response):\n",
    "    compare = pd.DataFrame(list(zip(x_test, y_test, pred)), columns = ['description', 'actual', 'predicted'])\n",
    "    compare = compare.assign(actual_product = encoder.inverse_transform(compare.actual),\n",
    "                             predicted_product = encoder.inverse_transform(compare.predicted)) \\\n",
    "                     .loc[(compare.actual == actual_response) & (compare.predicted == wrongly_predicted_response),\n",
    "                         ['description', 'actual_product', 'predicted_product']]\n",
    "    return compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-13T10:40:31.519Z"
    }
   },
   "outputs": [],
   "source": [
    "observe_errors(actual_response = 1, wrongly_predicted_response = 0).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the misclassified complaints shown above is quite ambiguous and contain ideas and keywords within the narrative related to both categories, which is presumably why the model would have misclassified them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afterward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, data preprocessing is an extremely important step when it comes to NLP problems and the \"garbage-in, garbage-out\" property can be very prevalent compared to other machine learning techniques. Also note that, in this problem that there are existing labels in the dataset, and we could have just as easily used any other supervised learning techniques to classify the documents. If however, no labels are available, we would then have to turn towards unsupervised learning algorithms such as SVM, K-means or LDA to try and cluster the documents into sensible categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have essentially just taught the computer how to \"understand\" a small selected group of documents in a very human way, which is just amazing! So, you might ask, how does this apply to actuarial work? <br>\n",
    "Imagine having access to all the historical claim narratives within your company's database. What do you think are some possible projects that will add value to your insurer now that you know how to model text? Could you maybe come up with a classifier using techniques above coupled with some clustering analysis techniques such as K-Means to categorize the claims if the legacy system does not have this feature? What about doing some sentiment analysis on the claim narratives to get the general satisfaction level of customers and to help pinpoint the strengths and weaknesses of the insurer's current claims handling process? <br>\n",
    "Do not be mistaken though, the possibilites are not constrained to the claims management section of the insurance value chain. Other examples include chatbots and recommendation systems as distribution channels, and fraud detection with text analysis. The possibilities are ever-growing, technology ever-improving and it is up to the actuarial profession to keep up with the trends in order to continue adding value to the industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TF-IDF Model](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) <br>\n",
    "[Python Regular Expressions (REGEX)](https://www.tutorialspoint.com/python/python_reg_expressions.htm) <br>\n",
    "[Logistic Regression](http://www.appstate.edu/~whiteheadjc/service/logit/intro.htm) <br>\n",
    "[Confusion Matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
